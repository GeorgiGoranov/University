{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c81c9f-099d-4ca2-8412-4cad2dc48310",
   "metadata": {},
   "source": [
    "Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29203e68-e830-4168-8f4b-5b36f5d1c4f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmnist_reader\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmnist_reader\u001b[39;00m\n\u001b[0;32m      2\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m mnist_reader\u001b[38;5;241m.\u001b[39mload_mnist(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/fashion\u001b[39m\u001b[38;5;124m'\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m mnist_reader\u001b[38;5;241m.\u001b[39mload_mnist(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/fashion\u001b[39m\u001b[38;5;124m'\u001b[39m, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt10k\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import utils.mnist_reader as mnist_reader\n",
    "X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af5fe73-4369-4f39-b3ba-07cedadc25ca",
   "metadata": {},
   "source": [
    "Train a Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12146bb7-97f2-4cfa-904e-75e1d678594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "clf.fit(X_train, y_train) # Train the classifier\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print training time\n",
    "training_time = end_time - start_time\n",
    "print(f\"Training time: {training_time} seconds\")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba7ce60-ad87-4ded-bca0-9a8fec93db23",
   "metadata": {},
   "source": [
    "use PCA to reduce the dataset's dimensionality (with an explained variance ratio of 95%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f19b2c8-2e42-4cbf-8347-f15b17b92608",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming X_train and X_test are already defined\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA(n_components=0.95)\n",
    "\n",
    "# Fit PCA on the training data\n",
    "pca.fit(X_train)\n",
    "\n",
    "# Transform both training and testing data\n",
    "X_train_reduced = pca.transform(X_train)\n",
    "X_test_reduced = pca.transform(X_test)\n",
    "\n",
    "# Print the reduction results\n",
    "print(f\"Original number of features: {X_train.shape[1]}\")\n",
    "print(f\"Reduced number of features: {X_train_reduced.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff405e-fcc3-4069-b765-0f6c108cfbff",
   "metadata": {},
   "source": [
    "Train a new Random Forest classifier on the reduced dataset and measure how long it takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bd414a-16aa-4acd-9aad-493ceea99ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "clf_reduced = RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "# Measure training time\n",
    "start_time = time.time()\n",
    "clf_reduced.fit(X_train_reduced, y_train)  # Train using the reduced dataset\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the training time\n",
    "training_time_reduced = end_time - start_time\n",
    "print(f\"Training time on the reduced dataset: {training_time_reduced:.3f} seconds\")\n",
    "\n",
    "# Evaluate the classifier on the reduced test set\n",
    "y_pred_reduced = clf_reduced.predict(X_test_reduced)\n",
    "accuracy_reduced = accuracy_score(y_test, y_pred_reduced)\n",
    "print(f\"Accuracy on the reduced test set: {accuracy_reduced * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e75547-a708-4a29-82e7-62e844c78eba",
   "metadata": {},
   "source": [
    "despite it being reduced it took longer than the frist dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89d1ba6-025a-4adc-9b00-6ec2bda13bae",
   "metadata": {},
   "source": [
    "While PCA can be a powerful tool for managing high-dimensional datasets, its impact on model training and performance can vary. These outcomes highlight the importance of empirically testing the effects of dimensionality reduction in the context of specific datasets and modeling tasks, rather than assuming universal benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087022b4-3746-45b7-a0fd-fd374327660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the softmax regression model on scaled data\n",
    "softmax_reg = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000)\n",
    "start_time = time.time()\n",
    "softmax_reg.fit(X_train_scaled, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the training time\n",
    "training_time_softmax = end_time - start_time\n",
    "print(f\"Training time for softmax regression on scaled data: {training_time_softmax:.3f} seconds\")\n",
    "\n",
    "# Evaluate the model on the scaled test set\n",
    "y_pred_softmax = softmax_reg.predict(X_test_scaled)\n",
    "accuracy_softmax = accuracy_score(y_test, y_pred_softmax)\n",
    "print(f\"Accuracy of softmax regression on the scaled test set: {accuracy_softmax * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92af984-7d44-4942-9287-c56d06eaf456",
   "metadata": {},
   "source": [
    "Apply softmax regression (using the reduced dataset) and time how long it takes, then evaluate the resulting model on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7076cb6c-acfd-4df8-bc44-b44674edc015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_reduced_scaled = scaler.fit_transform(X_train_reduced)\n",
    "X_test_reduced_scaled = scaler.transform(X_test_reduced)\n",
    "\n",
    "\n",
    "# Initialize and train the softmax regression model\n",
    "softmax_reg_reduced = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000, C=1e5)\n",
    "start_time = time.time()\n",
    "softmax_reg_reduced.fit(X_train_reduced_scaled, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the training time\n",
    "training_time_softmax_reduced = end_time - start_time\n",
    "print(f\"Training time for softmax regression on reduced dataset: {training_time_softmax_reduced:.3f} seconds\")\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred_softmax_reduced = softmax_reg_reduced.predict(X_test_reduced_scaled)\n",
    "accuracy_softmax_reduced = accuracy_score(y_test, y_pred_softmax_reduced)\n",
    "print(f\"Accuracy of softmax regression on the reduced test set: {accuracy_softmax_reduced * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33db19a0-e6be-4183-b18e-e5aa5fa38baa",
   "metadata": {},
   "source": [
    "\n",
    "From the results, it can be concluded that applying softmax regression to the reduced dataset significantly reduced the training time—from 232.169 seconds for the original dataset to just 31.005 seconds for the reduced dataset—while also slightly improving the accuracy, from 83.44% to 84.24%s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
