{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DAMI Assignment *\n",
    "\n",
    "# Who Survived the Titanic Disaster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Titanic example, we will use decision trees. The main advantage of this model is that humans can easily understand and reproduce the sequence of decisions taken to predict the target class of a new data point.\n",
    "\n",
    "This is very important for tasks such as medical diagnosis or credit approval, where we want to show a reason for the decision rather than just saying this is what the training data suggests (which is, by definition, what every supervised learning method does).\n",
    "\n",
    "The problem we would like to solve is determining whether a Titanic passenger would have survived, given their age, class, and sex.\n",
    "\n",
    "Why age, class and sex features?\n",
    "\n",
    "Answer: Particular features (the name is an extreme case) could result in overfitting (consider a tree that asks if the name is X; she survived). Features for which a small number of instances with each value present a similar problem. They might not be helpful for generalisation. We will use class, age, and sex because we expect them to have possibly influenced the passenger's survival.\n",
    "\n",
    "For this assignment, you need this Jupyter notebook and this dataset Download dataset. Have fun with data mining and building trees!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each instance in the dataset has the following form:\n",
    "\n",
    "     \"1\",\"1st\",1,\"Allen, Miss Elisabeth Walton\",29.0000,\"Southampton\",\"St Louis, MO\",\"B-5\",\"24160 L221\",\"2\",\"female\"\n",
    "     \n",
    "Note that the raw data consists largely of strings. To apply machine learning algo's these strings have to be converted to numerical data first (at least the columns that are of interest)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Dataset with Pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is a Python module that works with the so-called dataframe concept (rows are observations, columns refer to the features). A dataframe is essentially a two-dimensional labeled data structure where\n",
    "each column represent a feature and each row represents an observation.\n",
    "\n",
    "More details, see: https://www.kaggle.com/c/titanic/details/getting-started-with-python-ii "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Titanic dataset (csv file) from Canvas, read it with Pandas into a dataframe. Show the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print('numpy version:', np.__version__)\n",
    "print('matplotlib version:', pd.__version__)\n",
    "\n",
    "## Your code ...\n",
    "\n",
    "# read with pandas into a dataframe\n",
    "\n",
    "\n",
    "# show the first 5 rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also show the 10 last rows. What is the problem in the last couple of rows?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code ...\n",
    "\n",
    "# show 10 last rows\n",
    "\n",
    "# problem: ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Investigate Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does Pandas interpret the data? The following 3 commands can be used to investigate the data. Describe in your own words what the command does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.dtypes\n",
    "\n",
    "## Your answer ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.info()\n",
    "\n",
    "## Your answer ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.describe()\n",
    "\n",
    "## Your answer ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice and print the first 10 rows of the 'age' column. \n",
    "\n",
    "## Your code ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What kind of object is this 'age' column?   \n",
    "\n",
    "\n",
    "# Note: Single column is neither an numpy array, nor a pandas dataframe but rather a pandas-specific object called data Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the average age over all passengers?\n",
    "\n",
    "## Your code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next thing we'd like to do is look at more specific subsets of the dataframe. Slice the columns 'sex', 'pclass', \n",
    "# and 'age'.\n",
    "\n",
    "## Your code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First look at all of the missing 'age' values, because we will need to address them in our model if we hope to use \n",
    "# all the data for more advanced algorithms. To filter for missing values you can use:\n",
    "df_titanic[df_titanic['age'].isnull()][['sex', 'pclass', 'age']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we finish the initial investigation, let's use one other convenience function of pandas to derive a \n",
    "# histogram of any numerical column. \n",
    "import pylab as pyl\n",
    "df_titanic['age'].hist()\n",
    "pyl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside the parentheses of .hist(), you can also be more explicit about options of this function. Before you invoke \n",
    "# it, you can also be explicit that you are dropping the missing values of age:\n",
    "df_titanic['age'].dropna().hist(bins=16, range=(0,80), alpha = .5)\n",
    "pyl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Munging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Transform the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the values in the dataframe into the shape we need for machine learning. \n",
    "\n",
    "First of all, it's hard to run analysis on the string values of \"male\" and \"female\". \n",
    "Let's store this transformation into a new column 'Sex'. We have a precedent of analyzing the women first, so let's decide female = 0 and male = 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's store our transformation in a new column, so the original sex isn't changed. Show the first 3 instances.\n",
    "\n",
    "## Your code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for passenger class, make it numeric, in a new column PClass. Show the first 3 instances.\n",
    "\n",
    "## Your code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all males in the second class\n",
    "\n",
    "## Your code ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Deal with Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to deal with the missing values of age! Why? Simply because most machine learning will need a complete set of \n",
    "values in that column to use it. By filling it in with guesses, we'll be introducing some noise into a model, but if we can \n",
    "keep our guesses reasonable, some of them should be close to the historical truth (whatever it was...), and the overall \n",
    "predictive power of age might still make a better model than before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know the average age of all passengers (with valid age field) is 31.2 - we could fill in the null values with that mean value. But may be the median would be better? (to reduce the influence of a few rare 70- and 80-year olds?) The age histogram did seem positively skewed. These are the kind of decisions you make as you create your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the NaN (unknown) age values with a reasonable estimate. Do this in a new column 'AgeFill'\n",
    "\n",
    "# Optionally, if you like a bit more programming try this one ...\n",
    "# Use the mean age that was typical for males and females in each passenger class \n",
    "\n",
    "## Your code ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a categorical feature attribute: pclass. We already converted its three classes into 1, 2, and 3. This transformation implicitly introduces an ordering. \n",
    "\n",
    "As a final step, we will try a more general approach that does not assume an ordering. This is widely used to convert categorical classes into real-valued attributes. We will introduce an additional encoder and convert the class attributes into three new binary features, each of them indicating if the instance belongs to a feature value (1) or (0). This is called one hot encoding, and it is a very common way of managing categorical attributes for real-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic['FirstClass'] = df_titanic['pclass'].map( {'1st': 1, '2nd': 0, '3rd': 0} ).astype(int)\n",
    "df_titanic['SecondClass'] = df_titanic['pclass'].map( {'1st': 0, '2nd': 1, '3rd': 0} ).astype(int)\n",
    "df_titanic['ThirdClass'] = df_titanic['pclass'].map( {'1st': 0, '2nd': 0, '3rd': 1} ).astype(int)\n",
    "\n",
    "df_titanic.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finalize Dataset for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize pre-processing by turning this into a numerical feature set (dataframe titanic_X) and a numerical target column \n",
    "# (dataframe titanic_y)\n",
    "\n",
    "## Your code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_X.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_y.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing step is usually under-estimated in machine learning methods, but as we can see even in this very simple example, it can take some time to make data look as our methods expect. It is also very important in the overall machine learning process; if we fail in this step (for example, incorrectly encoding attributes, or selecting the wrong features), the following steps will fail, no matter how good the method we use for learning!!\n",
    "\n",
    "We are now ready for the implementation of decision trees in scikit-learn, as this algo expects as input a list of \n",
    "real-valued features, and the decision rules of the model would be of the form: Feature < value. \n",
    "For example, AgeFill < 20.0.\n",
    "\n",
    "Standardization (normalization) is not an issue for decision trees because the relative magnitude of features does not \n",
    "affect the classifier performance; so scaling is not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Training a Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to the interesting part; let's build a decision tree from our training data. \n",
    "# As usual, first separate training and testing data, and check the size of both sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Your code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we can create a new DecisionTreeClassifier and use the fit method of the classifier to do the learning job. \n",
    "# Parameter settings: use the entropy citerion and try out different settings for the depth of the tree and the \n",
    "# minimum samples required for a node in the tree graph\n",
    "from sklearn import tree\n",
    "\n",
    "## Your code ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Evaluation Metrics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generic helper function to measure the performance of the classifier, and call this function to show \n",
    "# the results (e.g. accuracy)\n",
    "from sklearn import metrics\n",
    "\n",
    "## Your code ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Introducing Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common criticism to decision trees is that once the training set is divided after\n",
    "answering a question, it is not possible to *reconsider this decision*. For example, if\n",
    "we divide men and women, every subsequent question would be only about men or\n",
    "women, and the method could not consider another type of question (say, age less\n",
    "than a year, irrespective of the gender). Random Forests try to introduce some level\n",
    "of randomization in each step, proposing alternative trees and combining them to\n",
    "get the final prediction. These types of algorithms that consider several classifiers\n",
    "answering the same question are called **ensemble methods**. In the Titanic task, it is\n",
    "probably hard to see this problem because we have very few features, but usually\n",
    "a case has in the order of thousand(s) features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests propose to build several decision trees, each one based on a subset of the training\n",
    "instances (selected randomly), and using a small random number of features. \n",
    "This produces multiple classifiers (multiple decision trees). \n",
    "At prediction time, each grown tree, given an instance, predicts its target class exactly as decision trees do. \n",
    "The class that most of the trees vote (that is the class most predicted by the trees) is the one suggested by the ensemble classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implement Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a Random Forest classifier. Can you improve to above accuracy? Look at the sklearn documentation. Play with the parameters of the ``RandomForestClassifier``. Especially the parameter ``n_estimators`` (the number of trees in the forest) is of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a Random Forest classifier, does this improve the prediction?\n",
    "\n",
    "## Your code ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
